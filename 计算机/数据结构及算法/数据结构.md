> 下面所有的内容参考自 [浙江大学_数据结构_陈越_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1JW411i731/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=2ebbcf68d17998bd601d8775237259ad)

> 本文中所有的算法和数据结构都会以C语言为例，但是不影响我们对这两个概念的理解。


# 基础

**什么是数据结构**
- 数据结构是在数据在计算机中的组织方式
- 数据对象必定与一系列加在其上的操作相关联。完成这些操作的方法就是**算法**。

## 算法

### 什么是算法

算法的性质应当满足以下条件：
- 一个有限的指令集
- 接受一个输入（有时不需要）
- 产生输出
- 有限的步骤内中止
- 每一条指令必须
   - 有充分明确的目标
   - 计算机可处理范围之内
   - 描述应不依赖于任何一种计算机语言及具体实现手段

描述算法效率的指标
- 时间复杂度
- 空间复杂度

### 时间复杂度

> 计算时间复杂度时，通常我们以循环作为主要参考，计算循环内的成本较高操作的次数来衡量<mark>（乘除 > 加减法）</mark>

在分析时间复杂度的时候，我们经常关心以下几种

1. 最坏时间复杂度
2. 平均时间复杂度
3. 最好时间复杂度

其中，我们又最为关注最坏时间复杂度。一方面，平均时间复杂度往往计算繁琐；另一方面，最坏时间复杂度代表最终的用户体验。

**常见的时间复杂度**

常见的时间复杂度有

1. 1
2. log（n
3. n
4. n*logn)
5. n^2
6. n^3
7. 2^n
8. n!

其它的几个复杂度都很容易想到相关的场景，下面举例说明下 log（n） 和 n * log(n) 复杂度的场景:
```
// log（n）
int i = 1;
while(i<n)
{
    i = i * 2;
}

// n*log(n)
for(m=1; m<n; m++)
{
    i = 1;
    while(i<n)
    {
        i = i * 2;
    }
}
```

> 一个算法的时间复杂度如果超过了 n^3 , 那么作为一个优秀的程序员，我们应该马上着手去优化它。一个 n*log（n） 的算法将会节省巨大的时间成本。

## 常见的算法设计
[五大常用算法——分治法，动态规划，回溯法，分支界限法，贪心算法-阿里云开发者社区 (aliyun.com)](https://developer.aliyun.com/article/284886)

#### 分治法
相关数据结构： 二叉搜索树、二叉平衡树

**介绍**
分治法的设计思想是：将一个难以直接解决的大问题，分割成一些规模较小的相同问题，以便各个击破，分而治之。

分治策略是：对于一个规模为 n 的问题，若该问题可以容易地解决（比如说规模 n 较小）则直接解决，否则将其分解为 k 个规模较小的子问题，这些子问题互相独立且与原问题形式相同，递归地解这些子问题，然后将各子问题的解合并得到原问题的解。这种算法设计策略叫做分治法。

分治法所能解决的问题一般具有以下几个特征：
1. 该问题的规模缩小到一定的程度就可以容易地解决
2. 该问题可以分解为若干个规模较小的相同问题，即该问题具有最优子结构性质。（此特征反映了递归思想的应用）
3. 利用该问题分解出的子问题的解可以合并为该问题的解；
4. 该问题所分解出的各个子问题是相互独立的，即子问题之间不包含公共的子子问题。

> 特征 3 是关键，能否利用分治法完全取决于问题是否具有第三条特征，如果具备了第一条和第二条特征，而不具备第三条特征，则可以考虑用贪心法或动态规划法
> 特征 4 涉及到分治法的效率，如果各子问题是不独立的则分治法要做许多不必要的工作，重复地解公共的子问题，此时虽然可用分治法，但一般用动态规划法较好。

**经典应用场景**
快速排序、归并排序，二分查找

#### 贪心法
相关数据结构： 图

**介绍**
贪心算法是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，他所做出的仅是在某种意义上的局部最优解。  

贪心算法没有固定的算法框架，算法设计的关键是贪心策略的选择。必须注意的是，贪心算法不是对所有问题都能得到整体最优解，选择的贪心策略必须具备无后效性，即某个状态以后的过程不会影响以前的状态，只与当前状态有关。

> 注意⚠️： 贪心策略适用的前提是：局部最优策略能导致产生全局最优解。贪心策略一定要仔细分析其是否满足无后效性! 
> 如果不满足，贪心法就不能使用！

贪心算法的基本思路：  
1. 建立数学模型来描述问题。  
2. 把求解的问题分成若干个子问题。  
3. 对每一子问题求解，得到子问题的局部最优解。  
4. 把子问题的解局部最优解合成原来解问题的一个解。

**经典应用场景**
背包问题、最优装载问题，图的最短路径


#### 动态规划法
相关数据结构： 图

**介绍**
基本思想与分治法类似，也是将待求解的问题分解为若干个子问题（阶段），按顺序求解子阶段，前一子问题的解，为后一子问题的求解提供了有用的信息。在求解任一子问题时，列出各种可能的局部解，通过决策保留那些有可能达到最优的局部解，丢弃其他局部解。依次解决各子问题，最后一个子问题就是初始问题的解。

 由于动态规划解决的问题多数有重叠子问题这个特点，为减少重复计算，对每一个子问题只解一次，将其不同阶段的不同状态保存在一个二维数组中。

与分治法最大的差别是：适合于用动态规划法求解的问题，经分解后得到的子问题往往不是互相独立的，即下一个子阶段的求解是建立在上一个子阶段的解的基础上，进行进一步的求解。


能采用动态规划求解的问题的一般要具有3个性质：
1. 最优化原理：如果问题的最优解所包含的子问题的解也是最优的，就称该问题具有最优子结构，即满足最优化原理。
2. 无后效性：即某阶段状态一旦确定，就不受这个状态以后决策的影响。也就是说，某状态以后的过程不会影响以前的状态，只与当前状态有关。
3. 有重叠子问题：即子问题之间是不独立的，一个子问题在下一阶段决策中可能被多次使用到。（该性质并不是动态规划适用的必要条件，但是如果没有这条性质，动态规划算法同其他算法相比就不具备优势）


在使用动态规划法进行算法设计的时候，我们一般遵循以下步骤：
1. 划分阶段：按照问题的时间或空间特征，把问题分为若干个阶段。在划分阶段时，注意划分后的阶段一定要是有序的或者是可排序的，否则问题就无法求解。
2. 确定状态和状态变量：将问题发展到各个阶段时所处于的各种客观情况用不同的状态表示出来。当然，状态的选择要满足无后效性。
3. 确定决策并写出状态转移方程：因为决策和状态转移有着天然的联系，状态转移就是根据上一阶段的状态和决策来导出本阶段的状态。所以如果确定了决策，状态转移方程也就可写出。但事实上常常是反过来做，根据相邻两个阶段的状态之间的关系来确定决策方法和状态转移方程。
4. 寻找边界条件：给出的状态转移方程是一个递推式，需要一个递推的终止条件或边界条件。


>动态规划法的求解过程可以用一个最优决策表来描述，最优决策表是一个二维表，其中行表示决策的阶段，列表示问题状态，表格需要填写的数据一般对应此问题的在某个阶段某个状态下的最优值，填表的过程就是根据递推关系，从1行1列开始，以行或者列优先的顺序，依次填写表格，最后根据整个表格的数据通过简单的取舍或者运算求得问题的最优解。


**经典应用场景**
背包问题、最长公共子序列问题、单源最短路径问题


#### 回溯法
相关数据结构： 图

**介绍**
回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。

在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根结点出发深度探索解空间树。当探索到某一结点时，要先判断该结点是否包含问题的解，如果包含，就从该结点出发继续探索下去，如果该结点不包含问题的解，则逐层向其祖先结点回溯。（其实回溯法就是对隐式图的深度优先搜索算法）。

用回溯法解题的一般步骤：  
1. 针对所给问题，确定问题的解空间：
2. 确定结点的扩展搜索规则
3. 以深度优先方式搜索解空间，并在搜索过程中用剪枝函数避免无效搜索



**经典应用场景**
	图的M着色问题、N皇后问题、计算机批处理作业调度


#### 分支界限法
相关数据结构： 图

**介绍**
在扩展结点处，先生成所有的子结点(分支)，然后从当前的活结点表中选择下一个扩展结点。为了有效的选择下一个扩展结点，以加速搜索的进程，在每一活结点处，计算一个函数值(限界)，并根据这些已经计算出的函数值，从当前活结点表中选择一个最有利的结点作为扩展结点，使搜索朝着解空间树上有最优解的分支推进，以便尽快找出一个最优解。

分支界限法以广度优先或以最小耗费(最大效益)优先的方式搜索问题的解空间树。问题的解空间树是表示问题解空间的一棵有序树，常见的有子集树和排列树。

**经典应用场景**
	单源最短路径问题


# 链表

### 链表的类型

**链表和数组对比**

- 链表
   - 各个节点在内存中的存储空间不一定是连续的，链表的作用就是将无序的节点串联起来
   - 方便插入。链表的前后节点可以随时修改，只要修改指针的指向就可以了。
   - 不方便查找。查找节点需要从头一个个往下寻找
- 数组
   - 占用的存储空间是连续的，占用的是一整块的内存。
   - 不方便插入。每次插入元素需要将后面所有的元素往后移一位。
   - 方便查找。通过下标可以直接寻找到元素。

**线性表**

线性表是由同类型的数据元素构成有序序列的线性结构

- 表中的元素个数称为线性表的长度
- 没有元素的线性表称为空表
- 表起始位置称为表头，结束称为表尾

**广义表**

广义表是线性表的拓展，广义表的节点不仅可以是单元素，也可以是另外一个广义表.

 
结构如下：

```
GNode {
    int Tag;
    union {
        ElementType Data;
        GList subList
    };
    GList Next;
}
```

**多重链表**

多重链表中的节点可能同时属于多个链

- 多重链表中节点的指针域会有多个
- 包含两个指针域的链表不一定是多重链表，如双向链表

**十字链表**

链表中存在两个指针域，将同行、同列串起来

- 行指针 Right
- 列指针 Dow

十字链表一个广泛的用途就是用来表示矩阵。比起使用数组表示它有以下优点：
- 只用存储非0项，比数组省空间
- 不用事先知道数组的大小，方便拓展

<img src="file:///Users/yangjiajian/Library/Application%20Support/marktext/images/2022-09-14-13-01-12-image.png" title="" alt="" width="499">

> 对于链表的使用场景，比较常见的就是多项式的表示法。
> 
> 线性表： P(x) = aX^n + b X^n2
> 
> 广义表： P(x, y) = a * X^i * Y^j + b * X^i2 * Y^j2
> 
>  多重链表： 树、图


### 堆栈

堆栈是具有一定操作约束的线性表 —— 只能在一端进行插入、删除。

- 堆栈通常用 Push（入栈）、Pop（出栈）两个方法进行操作。
- 堆栈的最重要特性就是后入先出（LIFO）

**堆栈的顺序存储**

堆栈的顺序存储结构通常由一个一维数组和一个记录栈顶元素位置的变量组成

**堆栈的链式存储**

堆栈的链式存储结构实际上就是一个单链表，插入和删除的操作只能在栈顶完成，所以栈顶指针应该指向链表的头部。

**堆栈的应用**

- 函数调用和递归（保存上一个函数的状态）
- 深度优先搜
- 回溯算法
- 。。。。

### 队列

队列是具有一定操作约束的线性表 —— 只能在一端进行插入、另一段进行删除。

- 堆栈通常用 Push（入列）、Pop（出列）两个方法进行操作。
- 堆栈的最重要特性就是先进先出（FIFO）

**队列的顺序存储**

队列的顺序存储通常由一个一维数组和一个记录队列头元素位置的变量 front 以及一个记录队列尾元素位置的变量 rear 组成

当我们重复利用数组的前面已出列的部分时，这种顺序存储的方式会使我们的队列呈现一个环状，有时我们也把这叫做**循环队列**。

这种情况下，我们其实无法判别队列是否是空的。（因为平常我们会判断 front === rear）。有两种方案解决这个问题

1. 使用额外标记。size 或者 tag 域
2. 仅使用 n-1 个数组空间

**队列的链式存储**

队列的链式存储结构可以用一个单链表来实现，插入和删除操作分别在链表的两端进行。

# 树

## 树的相关概念

**树的定义**

树是由 n（n  >= 0， 当 n = 0 时，称为空树）个节点构成的有限集合。

- 树中有一个节点称为根节点（Root），其余节点可分成 m 个互不相交的有限集，其中每个集合本身又是一棵树，称为原来树的子树。
- 子树之间互不相交
- 除根节点外，每个节点有且仅有一个父节点
- 一棵 n 个节点的子树有 n-1 条边

**树的相关概念**

- 节点的度： 节点的子树个树
- 树的度：树中所有节点中最大的度数
- 叶子节点
- 父节点
- 子节点
- 兄弟节点
- 祖先节点
- 子孙节点
- 路径和路径长度：从节点 N<sub>1</sub> 到 N<sub>k</sub> 的节点序列称为<u>节点路径</u>，路径中包含的边的条数称为<u>路径的长度</u>
- 节点的层次： 规定根节点为 1 层，其余节点的层数是其父节点的层次加 1
- 树的深度：树中所有节点的最大层次是这棵树的深度

## 树的表示法

**普通链表实现**

用普通链表实现，我们就需要保存一个节点信息，以及与树的度一样多的指针个数。但是其实每个节点的度有可能不一样，会导致空间的浪费。

![](/Users/yangjiajian/Library/Application%20Support/marktext/images/2022-09-15-12-54-14-image.png)


**儿子-兄弟表示法**

儿子-兄弟表示法，每个节点的数据结构需要保存一个节点信息，另外保存两个指针，一个指针指向该节点的第一个子节点（firstChild），另一个指针指向他的兄弟节点（next）。

![](/Users/yangjiajian/Library/Application%20Support/marktext/images/2022-09-15-12-57-42-image.png)

仔细观察，这样的数据结构形式可以看作每一个节点最多只有两个子节点，firstChild 视作左子节点，next 视作右子节点。由此引出我们对于**二叉树**的概念。

## 二叉树

二叉树是一个有穷的节点组合。可以为空。若不为空，则它是由根节点和称为其左子树和右子树的两个不相交的二叉树组成。

- 二叉树的子树有左右之分。一般的树是不强调节点的顺序的。

### 特殊的二叉树

- 斜二叉树
   - 所有的子树都是左子树或者所有的子树都是右子树
- 完美二叉树（满二叉树）
   - 除了最深层次的叶子节点子节点数为0外，所有层次的所有节点的子节点数目都为2
- **完全二叉树**
   - 有n个节点，对树中的节点从上到下，从左到右进行编号，任意编号位置的节点与满二叉树对应的位置相同。
      - 完全二叉树的性质：所有子树都是完全二叉树

<img src="file:///Users/yangjiajian/Library/Application%20Support/marktext/images/2022-09-15-22-04-33-image.png" title="" alt="" width="497">

### 二叉树的性质

- 二叉树第 i 层的最大节点数为 2 ^(i -1 )
- 深度为 k 的二叉树最大的节点数为 2 ^k - 1
	- 等比数列的求和公式 a * (1 - q<sup>n</sup>) / (1- q)
- 对于任何非空的二叉树 T，若 n0 表示叶子结点的个数，n2 表示深度为 2 的飞叶子结点的个数，那么 n0 = n2 + 1

完全二叉树的性质
- 若 i = 0, 则 i 无双亲,   若 i > 0, 则 i 的双亲为 (i -1)/2
- 若 2 * i + 1 < n, 则 i 的左子树序号为 2 * i，i 的右子树序号为 2 * i + 1
- 若结点编号 i 为偶数，且 i != 0,则左兄弟结点 i-1.
- 若结点编号 i 为奇数，且 i != n-1,则右兄弟结点为 i+1.
- 结点 i 所在层次为 log2(i+1) 
	- （为什么要 + 1 呢，因为根节点的序号是 0）


### 二叉树的存储结构

**顺序存储结构**

如果是一个<mark>完全二叉树</mark>，结点们之间是有顺序关系的。可以直接通过数组存储，一个下标为 i 节点的左右子节点的下标分别是 2i 和 2i + 1.父亲节点的下标为 i / 2 向下取整。

如果是一般的二叉树，那么我们需要将其补成完全二叉树，将不存在的节点设置特殊值表示。这样会造成极大的空间浪费。

**链式存储结构**

链式存储结构表示二叉树就非常简单了，只需要使用一个简单的链表就可以表示。链表的结构如下： 

```
TreeNode {
    Data data;
    BinTree Left;
    BinTree Right;
}
```

### 二叉树的遍历

二叉树的遍历形式主要有四种。

1. 先序遍历
   
    - 根节点 -> 左子树 -> 右子树

2. 中序遍历
   
    - 左子树 -> 根节点 ->  右子树

3. 后序遍历
   
    - 左子树 -> 右子树 -> 根节点

4. 层次遍历
   
    - 一层 -> 二层 -> .... -> n层

**递归遍历**

以中序遍历为例（先序、中序、后序实现方法一样，只是输出顺序不同）：

```
void InOrderTraversal(BinTree BT) {
    if (BT) {
       InOrderTraversal(BT -> Left)
        printf(BT -> Data)
        InOrderTraversal(BT -> Right)
    }
}
```

**非递归遍历**

非递归遍历的主要思路是使用堆栈。

以中序遍历为例：

1. 遇到一个节点先压入栈，然后遍历它的左子树

2. 左子树遍历结束之后，从栈顶抛出并输出

3. 然后遍历这个节点的右子树

```
void InOrderTraversal(BinTree BT) {
    BinTree T = BT;
    Stack S = CreateStack（MaxSize）
    while (T || !IsEmpty(S) {
        while (T) {
            Push(S, T);
            T = T -> Left
        }
        if (!IsEmpty(S)) {
            T = Pop(S);
            printf(T->Data);
            T = T -> Right;
        }
    }
}
```

**层次遍历**

层次遍历的核心问题是： 二维结构的线性化。

当进行层次遍历的时候，要考虑到访问了左儿子之后，右儿子如何继续进行的问题。

为了解决这个问题，我们需要一个结构保存暂时不访问的节点。比如堆栈、队列。以队列为例：

```
void LevelOrderTraversal(BinTreee Bt) {
    Queue Q;BinTree T;
    if (!BT) return ;
    Q = CreateQueue(MaxSize);
    AddQ(Q, BT);
    while (!IsEmpty(Q) {
        T = DeleteQ(Q);
        printf(T -> Data);
        if (T -> Left) AddQ(Q, T -> Left);
        if (T-> Right) AddQ(Q, T -> Right);
    }
}
```


**习题时间：**
```
中序遍历 inorder = [9,3,15,20,7]
后序遍历 postorder = [9,15,7,20,3]
```
根据两个遍历的结果，构建一颗二叉树



```
    3
   / \
  9  20
    /  \
   15   7
```





## 二叉搜索树

二叉搜索树： 一颗二叉树，可以为空；不为空时满足以下性质：

1. 非空左子树的所有值小于根节点
2. 非空右子树的所有值大于根节点
3. 左右子树都是二叉搜索树


一颗二叉搜索树通常拥有以下函数：
- Find
- FindMin
- FindMax
- Insert
- Delete



**二叉搜索树的删除**

```
BinTree Delete（ElementType X， BinTree BST）{
    Position Tmp;
    if(!BST) Printf('element is not found');
    else if (X < BST->Data) {
        BST-> Left = Delete(X, BST -> Left)
    }
    else if (X > BST->Data) {
        BST->Right = Delete(X, BST -> Right)
    }
    else {
        if (BST -> Left && BST -> Right) {
            Tmp = FindMin(BST -> Right);
            BST -> Data = Tmp -> Data;
            BST -> Right = Delete(BST -> Data, BST -> Right);
        } else {
            Tmp = BST;
            if (!BST -> Left) {
                BST = BST -> Right;
            } else if {
                BST = BST -> Right;
            }
            free(Tmp);
        }
    }
    return BST;
}
```



## 二叉平衡树

二叉搜索树的实现，对于同一个数据集，不同的组织方式，会导致不同的查找深度和<mark>平均查找长度（ASL）</mark>.
当二叉树是一个斜二叉树时，查找的深度和查找长度的期望值达到最大。



<mark>二叉平衡树（AVL树）</mark>

二叉平衡树可以为空树，若不为空树，则满足任意节点的左右子树的高度差绝对值不超过 1

![[Pasted image 20220926131949.png]]


**平衡二叉树的调整**

往平衡二叉树中插入新的节点时，会导致平衡二叉树变得不平衡了。

在对二叉树进行平衡的时候，需要采用“**<mark>旋转</mark>**”的方式对二叉树进行重新调整。



首先观察哪一个节点是不平衡的，这个节点称为”发现者“。插入的导致不平衡的节点称为“麻烦节点“。

节点的插入位置不同，需要旋转的位置和方向也就不同。我们用两个字母分别表示插入节点相对于“发现者”的位置。（如插入“发现者”的左子树的右边，则称为 LR插入 ，需要 LR旋转）



以 RR插入为例，下图表示 RR 插入的平衡过程：
![[Pasted image 20220926132235.png]]


todo：补充调整算法。




**习题时间**
1.完全二叉搜索树。
输入一个数字序列并构建一棵树，使得这一颗树既是二叉搜索树又是完全二叉树。并得出层序遍历的结果。
> 普通的二叉搜索树查找数字，构建的过程中，若树一直朝一边倾斜，会导致这颗树的节点搜索的深度和查找长度都很大。

像这样：
![[Pasted image 20220921224407.png]]

但是如果一颗树既是二叉搜索树又是一颗二叉完全树，那么效果就会好的多。像这样：
![[Pasted image 20220921224607.png]]


那么，我们构建这棵树的时候，用什么来表示这棵树比较好呢？
- 链表
	- 空间紧凑
	- 层序遍历比较麻烦，要用到队列
- 数组
	- 由于是完全二叉树，空间也很紧凑，空间没有浪费
	- 遍历非常方便，直接顺序输出

所以数组完胜！

构建这个二叉搜索完全树的过程：
1. 对 n 个数字的数字序列进行排序
2. 计算序列对应的左子树个数
	1. 满二叉树节点数为 2<sup>h</sup> - 1。计算 log<sub>2</sub>n 向上取整得到树的高度 h
	2. 计算 h - 1 层满二叉树的总节点数 k。 n - k 得到底层节点数 k1
	3. 底层中属于左子树的节点数为 2<sup>h</sup> / 2 = 2<sup>(h-1)</sup>.
	4. k1 小于左子树节点数，则左边有 k1 个节点。否则有2<sup>(h-1)</sup> 个节点
3. 递归处理

伪代码实现如下：

```
void solve(int ALeft, int ARight, int TRoot) {
	n = ARight - Aleft + 1;
	if (n === 0) return ;

	L = GetLeftLength(n);

	T(TRoot) = A[Left + L];
	LeftTRoot = TRoot * 2 + 1;
	RightRoot = LeftRoot + 1;

	solve(ALeft, ALeft + L - 1, LeftRoot);
	solve(ALeft + L + 1, ARight, RightRoot)
}
```



## 堆

堆是一个特殊的队列，从堆中取出元素的顺序是一找元素的优先权大小，而不是先后顺序

什么样的存储结构适合用来表示堆呢？
对于一个堆结构，我们通常只关心两个操作，一个是插入、一个删除。下面给出用数组和链表表示堆时算法的时间复杂度：
![[Pasted image 20220918002238.png]]

**用树来表示堆**
将所有节点构建成完全二叉树且同时又是一个二叉搜索树，此时的堆具有以下特性：
1. 结构性： 用数组表示的完全二叉树
2. 有序性： 任意节点的值是子树所有节点的极值
	- 极大值： 称为 最大堆
	- 极小值： 称为 最小堆


### 堆的插入和删除
以最大堆为例：

当插入新节点，会直接插入有序节点的最后一位。若此时新节点大于父节点，那么就需要交换二者的值，一直向上判断到根节点。
```
void Insert(MaxHeap H, ElementType item) {
	int i;
	if(IsFull(H)){
		Printf('Heap is full');
		return;
	}
	i = ++H->Size;
	for(;H->Elements[i/2] < item;i = i / 2) {
		H->Elements[i] = H -> Elements[i/2];
	}
	H -> Elements[i] = item
}
```

删除最大结点时，会将根节点不断往下移动，移动到最后进行删除
```
ElementType DeleteMax(MaxHeap H) {
	int Parent, Child;
	ElementType MaxItem, temp;
	if(IsEmpty(H)) {
		Printf('最大堆已空');
		return;
	}
	MaxItem = H->Element[1];
	temp = H->Element[H->Size --]; // 删掉最后一个并记住
	for(Parent = 1; Parent * 2 <= H->Size; Parent = Child) {
		Child = Parent * 2;
		if(Child != H->Size && (H->Elements[Child] < H->Element[Child+1])) {
			Child ++; // 左边大就不变，右边大就用右边
		}

		if (temp >= H-> Element[Child]) break;
		else 
			H->Elements[Parents] = H->Elements[Child];
		}
	}
	H->Elements[Parent] = temp;
	return MaxItem;
}
```


## 哈夫曼树

树是由 n（n  >= 0， 当 n = 0 时，称为空树）个节点构成的有限集合。
但是集合中的节点在构成数据的时候出现的频率可能是不一样的，如果我们能够将出现频率高的节点放在树的上层，会让搜索这个节点的效率有很大的提升。
![[Pasted image 20220918160507.png]]

**构建哈夫曼树**
构建哈夫曼树的过程，只要每次都将权值最小的两颗二叉树合并即可。构建后的结果会呈现一个最小堆的形式。
```
HuffmanTree Huffman(MinHeap H) {
	int i, HuffmanTree T;
	BuildMinHeap(H);
	for(i = 1; i < H -> Size; i ++) {
		T = malloc (sizeof(struct TreeNode));
		T -> Left = DeleteMin(H);
		T -> Right = DeleteMin(H);
		T -> Weight = T->Left->Weight + T->Right->Weight;
		Insert(H, T);
	}
	T = DeleteMin(H);
	return T;
}
```



**哈夫曼树的特点：**
- 没有度为1的节点
- n 个节点的哈夫曼树共有 2n - 1 个节点
- 哈夫曼树的左右子树交换后仍然是哈夫曼树



### 哈夫曼编码

#### 基础概念
**不等长编码**： 不同字符使用的存储空间位数不一样

**二义性**
例如下面的不等长编码
- a: 1
- b 0
- c: 10
- d: 11
那么当出现连续序列如 1011，就无法判断是哪些字符组成的。（有可能是 aeaa、aet、st）

那么如何避免二义性呢？
只要保证任何字符的编码都不是另一字符编码的前缀即可。

哈夫曼编码的特点：
1. 最优编码。总长度 WPL 最小。
2. 无歧义编码。任何字符的编码都不是另一字符编码的前缀，对应到树结构里就是说所有的字符编码都存在于叶子节点。
3. 没有度为 1 的节点。



#### 哈夫曼树的应用
给定一段字符串，对字符进行编码，如何才能使得该字符串的存储空间最少？
假设有一段文本，有 58 个字符，只含有7个字符（a,e,i,s,t,空格（sp）,换行（nl））。这七个字符出现的次数不同。
1. 等长 ascii 码： 58 * 8 = 464
2. 等长 3位编码： 58 * 3 = 174
3. 不等长编码


使用哈夫曼编码的例子
![[Pasted image 20220918163956.png]]


## 集合
感觉作用一般，跳过



# 图

**什么是图？**
- 表示多对多关系
- 包含
	- 一组顶点，通常用 V 表示
	- 一组边，通常用 E 表示
		- 边是顶点对的关联
			- 有向边
			- 无向边
		- 不考虑重边和自回路


图的分类
1. 无向图
	1. 边不具有方向性，纯粹关联两个顶点
2. 有向图
	1. 边具有方向性，只能一个顶点到另一个顶点



### 图的表示


#### 邻接矩阵
使用一个二维数组 G，若节点i到节点j存在边，则 G【i】【j】= 1。
![[Pasted image 20220918174920.png]]

**邻接矩阵的优缺点**
- 优点
	1. 直观简单
	2. 方便检查任意对顶点是否存在边
	3. 方便找任一顶点的“邻接点”
	4. 方便计算任一顶点的“度”（从该点出发的边称为“出度”，指向该点的边称为“入度”）
- 缺点
	- 浪费空间
	- 浪费时间


#### 邻接表
使用一个指针数组 G[N] ，数组每一项代表一个顶点，同时记录这个顶点相关联的顶点。
![[Pasted image 20220918175414.png]]

**邻接表的优缺点**
- 优点
	- 方便查找任一顶点的所有邻接点
	- 节约空间
	- 方便计算任一顶点的度
		- 对于无向图确实如此
		- 对于有向图，方便计算出度，但是入度计算会麻烦些
- 缺点
	- 不方便检查两个顶点之间是否存在边

## 图的遍历

**深度优先遍历**
深度优先遍历简称DFS（Depth First Search）。

从一个顶点开始，选定它的一个邻接点进行遍历，不断往后，每次只选定一个邻接点遍历。直到被遍历顶点的邻接顶点都已经被遍历过，则原路返回，到存在未被遍历完所有邻接节点，然后继续选定这个没被遍历过的节点进行遍历。直到所有的顶点都被遍历过，原路返回到第一个顶点。

> 类似于二叉树的前序遍历


**广度优先遍历**
广度优先遍历简称BFS（Breadth First Search）。

从一个顶点开始，同时遍历该顶点的所有邻接点，然后再遍历该顶点的所有邻接顶点的邻接顶点，直至所有的顶点都被遍历过。
 

## 关于图的更多概念
- 连通： 从顶点 v 到 顶点 w 之间存在一条路径，称 v 和 w 连通
- 路径： v 到 w 之间的路事一系列顶点的集合，其中任一对相邻的顶点间都存在图中的边。路径的长度就是路径中的边数（如果带权则是所有边的权重和）。如果 v 到 w 之间所有的顶点都不同，则称为简单路径。
- 回路：起点等于终点的路径
- 连通图：图中任意两个顶点都是连通的
- 连通分量： ==无向图==的极大连通子图
	- 极大顶点数：再加一个顶点就不连通了
	- 极大边数：包含子图中所有顶点相连的所有边
	- 
	- 示例如下，对于图 G，有几类它的子图，左边两个子图是连通分量，右边两个不是不是![[Pasted image 20220918225508.png]]
- 强连通：有向图中 v 和 w 之间存在双向路径，称 v 和 w 强连通
- 强连通图，有向图中，任意两个顶点都是强连通的
- 强连通分量： 有向图的极大强连通子图
	- 示例，对于图 G ，存在的两种强连通分量![[Pasted image 20220918230008.png]]




## 最短路径问题
> 想象一下你做地铁的时候该如何去选择你坐几号线


无权图的单源最短路径
类似于前面说的广度优先遍历BFS，一层层向外延伸，每次记录当前节点到开始节点的路径长度以及前一个节点的位置。

伪代码如下：
```
void UnWeighted (Vertex S) {
	Enqueue(S, Q);
	while(!IsEmpty(Q)) {
		V = Dequeue(Q);
		for(V 的每一个邻接点W) {
			if (dist[W] == -1) {
				dist[W] = dist[V] + 1; //距离+1
				path[W] = V; // 前一个节点
				Enqueue(W, Q);
			}
		}
	}
}
```


有权图的单源最短路算法
Dijkstra算法
1. 建立一个集合 S，集合中只包含源点 s
2. 遍历源点 s 所有邻接顶点，并更新这些顶点到 s 的距离到 dist 中
3. 找出 dist 数组中最小的一个顶点 v1 收录到集合 S 中，将 v1 标记为已收录（贪心法）
4. 遍历v1 的所有未收录邻接顶点，更新其到源点 s 的距离到dist，并记录对应的前一个顶点到 path
5. 重复第 3、4 步直到所有的顶点都已被收录


思考一下，下面这个图我们会如何是如何的一个收录顺序
![[Pasted image 20220919225558.png]]

顺序应当为： v1 - v4 - v2 - v3 - v5 - v7 - v6 


- 稠密图： 边的条数 E 与顶点数 V 相差至少一个数量级。
- 稀疏图： 边的条数 E 与顶点数 V 在同一个数量级。


对于 Dijkstra 算法中集合 S 收录新顶点的过程中，不同的收录方法有不同的适用场景
![[Pasted image 20220919230740.png]]





 Dijkstra 算法伪代码实现:
 ```
void Dijkstra(Vertex s) {
	while(1) {
		V = 未收录顶点中 dist 最小者；
		if (!V) break;
		collected[V] = true;
		for (v 每个邻接点 W) {
			if (!collected[W] && (dist[V] + E(v,w) < dist[W])) {
				dist[W] = dist[V] + E(v,w);
				path[w] = v;
			}
		}
	}
}
```





多源最短路径

计算多源最短路径的最简单方法就是直接将单源最短路径算法执行 V 次。但是这样子的时间复杂度就会达到 O(V<sup>3</sup> + E * V)。前面我们说过，任何一个优秀的程序员都不应该允许 N<sup>3</sup> 数量级复杂度的算法出现。

![[Pasted image 20220921130015.png]]

Floyd 算法的伪代码实现：

```
void Floyd() {
	for (i = 0; i < N; i ++) {
		for (j = 0; j < N; j ++) {
			D[i][j] = G[i][j];
			path[i][j] = -1;
		}
	}
	for (k = 0; k < N; k ++) {
		for (i = 0; i < N; i ++) {
			for (j = 0; j < N; j ++) {
				if (D[i][k] + D[k][j] < D[i][j]) {
					D[i][j] = D[i][k] + D[k][j];
					path[i][j] = k;
				}
			}
		}
	}
}
 ```



# 散列表
## 什么是散列表

学到这里，我们来回顾一下我们前面已经学过的几种查找方法
1. 顺序查找。O(N)
2. 二分查找。O(log<sub>2</sub>N)
3. 二叉搜索树。O(h)
4. 二叉平衡树。O(log<sub>2</sub>N)

看下面这个列子：
![[Pasted image 20220921235605.png]]



查找的本质，其实就是从已知的对象里面找目标的位置。为了提高我们查找的效率，我们要合理的组织我们存储的数据：
- 有序安排对象。全序、半序
- 直接“算”出对象的位置。 散列！


散列查找法的基本思路就是：
- 提前计算好目标应该处于我们的存储对象的哪一个位置。为了实现这一目标，我们要构建一个特殊的散列函数。
- 解决冲突。保证多个目标在我们的对象中位置重叠时还能规整的处置他们。

如果能够实现这样的数据结构和函数，我们通过散列查找法查找目标的时间复杂度几乎是常量级别 —— O(1) !

	> 设计这样的散列函数，需要考虑两个因素
	> 1. 计算简单。以便于提高转换速度。
	> 2. 计算词对应的地址空间分布均匀，以减少冲突。

### 散列中的基本概念
- 装填因子
	- 关键词序列的总量占散列空间的比例。
- 再散列
	- 当散列表中的元素太多了的时候（即装填因子 a 太大），查找的效率会下降；
		- 实用最大装填因子一般取 0.5 - 0.85
	- 当装填因子过大时，解决的方法是加倍扩大散列表，这个过程叫做再散列。
		- 再



思考一下，如果我们的数据是数字，那么我们就会很容易的去组织找到它。比如说我们要找 1001，那么我们就直接可以知道我们要去第1001 个位置找这个数字。但是同样的问题对于字符串或者其它就会难的多。
那么，我们能不能像数字那样去比较字符串，从而直接去合适的位置找到这个字符串呢？



**数字的散列函数举例**
1. 直接定址法。直接去关键词某个线性函数作为散列地址。
2. 除留余数法。h(key) = key % p (p 一般为空间大小)
3. 数字分析法。分析数字关键字在各位上的变化情况，取比较随机的数字位作为散列地址。
	1. 例如取手机号的后 4 位
4. 折叠法。将关键词话费成位数相同的几个部分然后叠加。
	1. 如将 56793542 按三位拆开组合，结果会是 056 + 793 + 542 = 1391
5. 平方取中法。
	1. 例如数字 n， 结果会是 n * n / 2



下面让我们更进一步，来看一下字符的散列函数构造。

**字符的散列函数举例**
1. ASCII 码加和法。将所有字符对应的 ASCII 码值简单相加，然后用数字的除留余数法计算。
	1.冲突会很严重！
2. 前三个字符移位法。先对前三个字符进行移位—— （key[0] * 27<sup>2</sup> + key[1] * 27 + key[2] ），然后用数字的除留余数法计算。这是对 ASCII 码加和法 的一个简单改进，可以使冲突有效减少。
	1. 为什么是27呢，因为出现最多的字符是 26 个英文字母和 1 个空格。
	2. 仍然存在不少冲突，并且会浪费大量的空间。
3. 移位法。


## 解决冲突的方法

解决散列中冲突的常用思路：
1. 换个位置——开放定址法
2. 同一个位置的冲突组织在一起——分离链接法


### 开放定址法
一旦产生了冲突，那么就按某种规则去寻找另一个空地址。

开放定址法的思路就是，若发生了第 i 次冲突，那么就将试探的下一个地址增加 d<sub>i</sub>，基本的公式是：
> h<sub>i</sub>(key) = (h(key) + d<sub>i</sub>) mod TableSize

d<sub>i</sub> 的不同决定了不同的解决冲突方案。
1. 线性探测。d<sub>i</sub> = i
2. 平方探测。di = +- i<sup>2</sup>
3. 双散列。d<sub>i</sub> = i * h<sub>2</sub>(key)


#### 线性探测
以 d<sub>i</sub> = 1 为例，散列表长为 13 ，构建序列{ 47， 7， 29， 11，9， 84， 54，20， 30} 的散列表。

h(key) = key mod ==11==
> 除数不一定等于表大小，留有余地可以更好的容纳冲突
![[Pasted image 20220922213554.png]]

**聚集**： 线性探测法一般来说性能会比较差。因为一旦某一个区域被占用，那么后续发生冲突，就会不停的在这块区域旁边向旁边延伸，会导致一个聚集的效应



#### 散列表查找性能分析
散列表查找的性能可以分为两个部分：
- 成功平均查找长度（ASLs）
- 不成功平均查找长度（ASLu)

以上面例子构造的散列表为例：![[Pasted image 20220922214954.png]]
成功平均查找长度是每一个元素需要找的次数的平均。
ASLs = (1 + 7 + 1 + 1 + 2 + 1 + 4 + 2 + 4) / 9 = 2.56

不成功平均查找长度是每一种余数的可能，到散列表中查找，直到发现不存在与散列表中的查找次数平均。
例如，对于余数为 0 的数，但是 0 和 1 的位置都已经被占用了，按照线性探测法的 d<sub>i</sub> = 1 往下找，找到位置 2 才会知道这个数在我们构造的散列表中是不存在的。

ASLu = （3 + 2 + 1 + 2 + 1 + 1 + 1 + 9 + 8 + 7 + 6）/ 11 = 3.73



#### 平方探测
平方探测法： 以增量序列 1<sup>2</sup>, -1<sup>2</sup>,2<sup>2</sup>, -2<sup>2</sup>,3<sup>2</sup>, -3<sup>2</sup>,..........q<sup>2</sup>, -q<sup>2</sup> （q <= TableSize / 2） 的循环去试探下一个存储地址。

以表长为 11 ，关键词序列为 { 47, 7, 29, 11, 9, 84, 54, 20, 30} 序列为例，构造散列表：
> h(key) = key mod 11

![[Pasted image 20220922220835.png]]

按照上面的方式对这个散列表进行性能分析：
ASLs = (1 + 1 + 2 + 1 + 1 + 3 + 1 + 4 + 4) / 9 = 2

平方探测法也有一个很致命的缺点，那就是平方探测法不一定能探测到一个数的位置。例如下面这个散列表，查找余数为 1 的元素，就会不断在表内位置跳跃，无法找到准确的位置：
![[Pasted image 20220922222758.png]]


**平方探测法的实现**
当我们单纯的构造一个一列的散列表时，这对于插入和删除来说是没有问题的，但是删除的时候我们就没有办法去直接把这个元素删除了。
因为我们在前面也说过，我们在查找一个元素的时候，如果直接查没找到就会去它的左右找。==但是我们如果实现了删除操作，如果一个节点被删除，我们就会认为这个地方是没有元素的，我们就会认为找不到这个元素。==
所以，我们在用数据结构去表示散列表的时候我们要构造两列的散列表，一列用来表示数据，一列用来表示这个数据的状态。




下面来介绍散列表的伪代码实现：

```
struct HashTbl {
	int TableSize;
	Cell *TheCells;
}

HashTable InitHashTable(int TableSize) {
	HashTable H;
	int i;
	if (Table < MinTableSize) {
		Printf('散列表太小');
		return null;
	}
	H = (HashTable)malloc(sizeof(struct HashTbl));
	if (H === Null) {
		throw Error('空间溢出');
	}
	H ->TableSize = NextPrime(TableSize);
	H->TheCells = (Cell *)malloc(sizeof(Cell)*H -> TableSize);
	if (H->TheCells === Null) {
		throw Error('空间溢出');
	}
	for(i = 0; i < H->TableSize; i++) {
		H->TheCells[i].info = Empty;
	}
	return H;
}
```


```
Position Find(ElementType Key, HashTable H) {
	Position CurrentPos, NewPos;
	int CNum; // 冲突次数
	NewPos = CurrentPos = Hash(Key, H -> TableSize);
	while(H->TheCells[NewPos].info !== Empty && H->TheCells[NewPos].Element !== Key) {
		if(++CNum % 2) { // 判断奇偶次
			NewPos = CurrenPos + (CNum + 1)/2*(CNum + 1)/2;
			while(NewPos >= H->TableSize) {
				NewPos -= H->TableSize;
			}
		} else {
			NewPos = CurrentPos - CNum/2 * CNum/2;
			while(NewPos < 0) {
				NewPos += H -> TableSize;
			}
		}
	}
	return NewPos;
}
```


```
void Insert(ElementType Key, HashTable H) {
	Position Pos;
	Pos = Find(Key, H);
	if(H -> TheCells[Pos].info !== Legitimate) {
		H -> TheCells[Pos].info = Legitimate;
		H -> TheCells[Pos].Elements = Key;
	}
}
```





### 分离链接法
将相应位置上的所有冲突的关键词都存储在同一个单链表中。

分离链接法查找元素的伪代码：
```
Position Find(ElementType Key, HashTable H) {
	Position P;
	int Pos;
	
	Pos = Hash(Key, H ->TableSize);
	P = H->TheList[Pos].Next;
	while (P!==Null && strcmp(P->Element, Key)) {
		P = P->Next;
	}
	return P;
}
```


使用分离链接法来组织散列表，采用顺序存储和链式存储相结合，这样的数据结构能够完美解决冲突问题。
但是链表部分的存在会导致存储效率和查找效率都变得比较低


## 总结
- 选择合适的 h(key) ，散列表的查找效率期望是常数级别的，它几乎与关键词的空间的大小 n 无关，也十分适合关键词直接比较计算量大的问题
- 散列表是以较小的 a 为前提。因此散列方法是一个空间换时间的数据结构。
- 散列方法的存储对于关键字是随机的，不便于顺序查找关键字，也不适合进行范围查找/最大最小值查找。